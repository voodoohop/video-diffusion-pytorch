{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMJmGNFEEgmOxlFC41aF4lC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/voodoohop/video-diffusion-pytorch/blob/main/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQXXHlzonawH"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/voodoohop/video-diffusion-pytorch.git\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from video_diffusion_pytorch import Unet3D, GaussianDiffusion, Trainer\n",
        "\n",
        "model = Unet3D(\n",
        "    dim = 64,\n",
        "    dim_mults = (1, 2, 4, 8),\n",
        ")\n",
        "\n",
        "diffusion = GaussianDiffusion(\n",
        "    model,\n",
        "    image_size = 64,\n",
        "    num_frames = 10,\n",
        "    timesteps = 100,   # number of steps\n",
        "    loss_type = 'l1'    # L1 or L2\n",
        ").cuda()\n",
        "\n",
        "trainer = Trainer(\n",
        "    diffusion,\n",
        "    '/content/frames',                         # this folder path needs to contain all your training data, as .gif files, of correct image size and number of frames\n",
        "    train_batch_size = 8,\n",
        "    train_lr = 2e-5,\n",
        "    save_and_sample_every = 100,\n",
        "    train_num_steps = 700000,         # total training steps\n",
        "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
        "    ema_decay = 0.995,                # exponential moving average decay\n",
        "    amp = True                        # turn on mixed precision\n",
        ")\n",
        "import wandb\n",
        "#wandb.init()\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "c-nyuoU-5QDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.flatten(torch.tensor([[1,2,3],[1,2,3]]),end_dim=1).shape"
      ],
      "metadata": {
        "id": "ImSpwtTbN4tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from video_diffusion_pytorch import Unet3D, GaussianDiffusion\n",
        "\n",
        "# model = Unet3D(\n",
        "#     dim = 64,\n",
        "#     dim_mults = (1, 2, 4, 8)\n",
        "# )\n",
        "\n",
        "# diffusion = GaussianDiffusion(\n",
        "#     model,\n",
        "#     image_size = 32,\n",
        "#     num_frames = 5,\n",
        "#     timesteps = 1000,   # number of steps\n",
        "#     loss_type = 'l1'    # L1 or L2\n",
        "# )\n",
        "\n",
        "videos = torch.randn(1, 3, 10, 64, 64).cuda() # video (batch, channels, frames, height, width)\n",
        "loss = diffusion(videos)\n",
        "loss.backward()\n",
        "# after a lot of training\n",
        "\n",
        "sampled_videos = diffusion.sample(batch_size = 4)\n",
        "sampled_videos.shape # (4, 3, 5, 32, 32)"
      ],
      "metadata": {
        "id": "v8E7KLXknoN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub sacremoses transformers sentencepiece\n",
        "import torch\n",
        "from video_diffusion_pytorch import Unet3D, GaussianDiffusion\n",
        "\n",
        "# model = Unet3D(\n",
        "#     dim = 64,\n",
        "#     use_bert_text_cond = True,  # this must be set to True to auto-use the bert model dimensions\n",
        "#     dim_mults = (1, 2, 4, 8),\n",
        "# )\n",
        "\n",
        "# diffusion = GaussianDiffusion(\n",
        "#     model,\n",
        "#     image_size = 32,    # height and width of frames\n",
        "#     num_frames = 5,     # number of video frames\n",
        "#     timesteps = 1000,   # number of steps\n",
        "#     loss_type = 'l1'    # L1 or L2\n",
        "# )\n",
        "\n",
        "videos = torch.randn(3, 3, 5, 32, 32) # video (batch, channels, frames, height, width)\n",
        "\n",
        "text = [\n",
        "    'fireworks with blue and green sparkles'\n",
        "]\n",
        "\n",
        "loss = diffusion(videos, cond = text)\n",
        "loss.backward()\n",
        "# after a lot of training\n",
        "\n",
        "sampled_videos = diffusion.sample(cond = text, cond_scale = 2)\n",
        "sampled_videos.shape # (3, 3, 5, 32, 32)"
      ],
      "metadata": {
        "id": "IYN6ZaIt6HMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms as T, utils\n",
        "def video_tensor_to_gif(tensor, path, duration = 80, loop = 0, optimize = True):\n",
        "    images = map(T.ToPILImage(), tensor.unbind(dim = 1))\n",
        "    first_img, *rest_imgs = images\n",
        "    first_img.save(path, save_all = True, append_images = rest_imgs, duration = duration, loop = loop, optimize = optimize)\n",
        "    return images\n",
        "\n",
        "video_tensor_to_gif(sampled_videos[0], \"/content/vid.gif\")"
      ],
      "metadata": {
        "id": "1amvdO86objv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "CHANNELS_TO_MODE = {\n",
        "    1 : 'L',\n",
        "    3 : 'RGB',\n",
        "    4 : 'RGBA'\n",
        "}\n",
        "\n",
        "def seek_all_images(img, channels = 3):\n",
        "    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n",
        "    mode = CHANNELS_TO_MODE[channels]\n",
        "\n",
        "    i = 0\n",
        "    while True:\n",
        "        try:\n",
        "            img.seek(i)\n",
        "            yield img.convert(mode)\n",
        "        except EOFError:\n",
        "            break\n",
        "        i += 1\n",
        "\n",
        "def gif_to_tensor(path, channels = 3, transform = T.ToTensor()):\n",
        "    img = Image.open(path)\n",
        "    tensors = tuple(map(transform, seek_all_images(img, channels = channels)))\n",
        "\n",
        "    split = torch.split(torch.stack(tensors, dim = 1), 10, dim = 1)\n",
        "    split = split[0:len(split)-1]\n",
        "\n",
        "    return torch.stack(split)\n",
        "\n",
        "gif_to_tensor(\"/content/frames/ezgif.com-gif-maker (3).gif\").shape"
      ],
      "metadata": {
        "id": "9e3yrVh3o0Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = [1,2]\n",
        "p*10"
      ],
      "metadata": {
        "id": "z0AWay00F6q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2_UqPRIfJ8Us"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}